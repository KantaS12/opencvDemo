{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_unzip(url, save_path):\n",
    "    print(f\"Downloading and extracting assests....\", end=\"\")\n",
    "\n",
    "    urlretrieve(url, save_path)\n",
    "\n",
    "    try:\n",
    "        with ZipFile(save_path) as z:\n",
    "            z.extractall(os.path.split(save_path)[0])\n",
    "\n",
    "        print(\"Done\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nInvalid file.\", e)\n",
    "URL = r\"https://www.dropbox.com/s/qa1hsyxt66pvj02/opencv_bootcamp_assets_NB10.zip?dl=1\"\n",
    "\n",
    "asset_zip_path = os.path.join(os.getcwd(), \"opencv_bootcamp_assets_NB10.zip\")\n",
    "\n",
    "if not os.path.exists(asset_zip_path):\n",
    "    download_and_unzip(URL, asset_zip_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Idea:\n",
    "1. Dynamic range of images is limited to 8-bits (0 - 255) per channel\n",
    "2. very bright pixels saturate to 255\n",
    "3. very dark pixels clip to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Capture Multiple Exposures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImagesAndTimes():\n",
    "    filenames = [\"img_0.033.jpg\", \"img_0.25.jpg\", \"img_2.5.jpg\", \"img_15.jpg\"]\n",
    "    times = np.array([1 / 30.0, 0.25, 2.5, 15.0], dtype=np.float32)\n",
    "    images = []\n",
    "    for filename in filenames:\n",
    "        im = cv2.imread(filename)\n",
    "        images.append(im)\n",
    "    return images, times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@0.411] global loadsave.cpp:241 findDecoder imread_('img_0.033.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@0.411] global loadsave.cpp:241 findDecoder imread_('img_0.25.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@0.411] global loadsave.cpp:241 findDecoder imread_('img_2.5.jpg'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@0.411] global loadsave.cpp:241 findDecoder imread_('img_15.jpg'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m images, times \u001b[38;5;241m=\u001b[39m readImagesAndTimes()\n\u001b[1;32m      3\u001b[0m alignMTB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcreateAlignMTB()\n\u001b[0;32m----> 4\u001b[0m alignMTB\u001b[38;5;241m.\u001b[39mprocess(images, images)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /Users/xperience/GHA-Actions-OpenCV/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "images, times = readImagesAndTimes()\n",
    "\n",
    "alignMTB = cv2.createAlignMTB()\n",
    "alignMTB.process(images, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "images, times = readImagesAndTimes() reads the images and then the exposure times too.\n",
    "\n",
    "The second half aligns the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Estimate Camera Response Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m calibrateDebevec \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcreateCalibrateDebevec()\n\u001b[0;32m----> 2\u001b[0m responseDebevec \u001b[38;5;241m=\u001b[39m calibrateDebevec\u001b[38;5;241m.\u001b[39mprocess(images, times)\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m256\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueez(responseDebevec)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
     ]
    }
   ],
   "source": [
    "calibrateDebevec = cv2.createCalibrateDebevec()\n",
    "responseDebevec = calibrateDebevec.process(images, times)\n",
    "\n",
    "x = np.arange(256, dtype=np.uint8)\n",
    "y = np.squeez(responseDebevec)\n",
    "\n",
    "ax = plt.figure(figsize=(30, 10))\n",
    "plt.title(\"Debevec Inverse Camera Response Function\", fontsize=24)\n",
    "plt.xlabel(\"Measured Pixel Value\", fontsize=22)\n",
    "plt.ylabel(\"Calibrated Intensity\", fontsize=22)\n",
    "plt.xlim([0, 260])\n",
    "plt.grid()\n",
    "plt.plot(x, y[:, 0], \"b\", x, y[:, 1], \"g\", x, y[:, 2], \"r\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Merge Exposure into an HDR Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'responseDebevec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m mergeDebevec \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcreateMergeDebevec()\n\u001b[0;32m----> 2\u001b[0m hdrDebevec \u001b[38;5;241m=\u001b[39m mergeDebevec\u001b[38;5;241m.\u001b[39mprocess(images, times, responseDebevec)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'responseDebevec' is not defined"
     ]
    }
   ],
   "source": [
    "mergeDebevec = cv2.createMergeDebevec()\n",
    "hdrDebevec = mergeDebevec.process(images, times, responseDebevec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges images into a HDR Linear image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Tonemapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonemapDrago = cv2.createTonemapDrago(1.0, 0.7)\n",
    "ldrDrago = tonemapDrago.process(hdrDebevec)\n",
    "ldrDrago = 3 * ldrDrago\n",
    "cv2.imwrite(\"ldr-Drago.jpg\", 255*ldrDrago)\n",
    "plt.figure(figsize=(20, 10));plt.imshow(np.clip(ldrDrago, 0, 1)[:,:,::-1]);plt.axis(\"off\");\n",
    "#Using Drago Method to obtain 24-bit color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tonemaping using Reinhard's method ... \")\n",
    "tonemapReinhard = cv2.createTonemapReinhard(1.5, 0, 0, 0)\n",
    "ldrReinhard = tonemapReinhard.process(hdrDebevec)\n",
    "cv2.imwrite(\"ldr-Reinhard.jpg\", ldrReinhard * 255)\n",
    "plt.figure(figsize=(20, 10));plt.imshow(np.clip(ldrReinhard, 0, 1)[:,:,::-1]);plt.axis(\"off\")\n",
    "#Using Reinhard's method to obtain 24-bit color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tonemaping using Mantiuk's method ... \")\n",
    "tonemapMantiuk = cv2.createTonemapMantiuk(2.2, 0.85, 1.2)\n",
    "ldrMantiuk = tonemapMantiuk.process(hdrDebevec)\n",
    "ldrMantiuk = 3 * ldrMantiuk\n",
    "cv2.imwrite(\"ldr-Mantiuk.jpg\", ldrMantiuk * 255)\n",
    "plt.figure(figsize=(20, 10));plt.imshow(np.clip(ldrMantiuk, 0, 1)[:,:,::-1]);plt.axis(\"off\")\n",
    "#Using Mantiuk's method to obtain 24-bit color image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of cv2.createAlignMTB is to create an object for aligning images based on brightness differences.\n",
    "\n",
    "The purpose of the tone mapping process is to reduce the dynamic range of HDR image for display on a low dynamic range monitor.\n",
    "\n",
    "cv2.createMergeDebevec handles over and under exposed pixels by using an adaptive weighting function\n",
    "\n",
    "The potential issue when using cv2.createMergeDebevec is the motion blur in the imput images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
